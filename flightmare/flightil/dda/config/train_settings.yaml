log_dir: "dda/results/testing"
quad_name: "none"
seq_len: 1 # History Length. When increasing to more than one, the model becomes machine dependent due to data saving latency.
checkpoint:
  # Put to true and add a path for loading a ckpt
  resume_training: False
  resume_file: ""
train:
  gpu: 0
  max_training_epochs: 150
  max_allowed_error: 2.0
  batch_size: 1
  learning_rate: 0.0001
  summary_freq: 400
  min_number_fts: 40
  train_dir: "dda/data/testing/train"
  val_dir: "dda/data/testing/test"
  save_every_n_epochs: 5
  use_imu: True
  use_fts_tracks: False
  use_pos: False
  use_activation: False
  attention_fts_type: "none"  # chose from "none", "decoder_fts", "map_tracks", "gaze_tracks"
  attention_model_path: "/home/simon/dda-inputs/attention_model_0/model/attention_model_0.pt"
  shallow_control_module: False
